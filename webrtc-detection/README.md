# WebRTC Real-time Multi-Object Detection

Real-time object detection demo that streams video from phone via WebRTC, performs YOLO inference, and overlays bounding boxes in near real-time.

## Requirements

- **Node.js 20+** (recommended for best compatibility)
- Docker & Docker Compose (optional)
- Chrome/Safari browser on phone
- Same WiFi network (or ngrok for external access)

## Quick Start

### One-Command Start
```bash
# Clone and start (defaults to WASM mode)
git clone <your-repo>
cd webrtc-detection
./start.sh
```

### Mode Selection
```bash
# WASM mode (low-resource, runs on modest laptops)
MODE=wasm ./start.sh

# Server mode (requires more resources, better accuracy)  
MODE=server ./start.sh

# With ngrok for external phone access
./start.sh --ngrok
```

### Docker Alternative
```bash
docker-compose up --build
```

### Using Docker
```bash
docker-compose up --build
```

## Phone Connection

1. Start the server with `./start.sh`
2. Open `http://localhost:3000` in your browser
3. Scan the displayed QR code with your phone
4. Allow camera access on your phone
5. Watch live object detection overlays!

## Development

```bash
# Install dependencies
npm install

# Run in development mode with auto-restart
npm run dev

# Run production mode
npm start
```

## Benchmarking

Run 30-second benchmark:
```bash
./bench/run_bench.sh --duration 30 --mode wasm
cat metrics.json
```

## Troubleshooting

- **Node.js version**: Ensure you're using Node.js 20+ for best compatibility
- **Phone won't connect**: Ensure same WiFi network or use `./start.sh --ngrok`
- **High CPU**: Switch to WASM mode or reduce resolution
- **Misaligned overlays**: Check browser console for timestamp errors
- **Connection issues**: Use Chrome DevTools > webrtc-internals
- **Module errors**: Make sure you're using Node.js 20+ with ES modules support

## Architecture

- **Frontend**: Vanilla JS + WebRTC
- **Backend**: Node.js 20 + Express 5 + Socket.IO 4.8
- **ML Model**: YOLOv5n (quantized)
- **Modes**: Server inference vs WASM browser inference

## Performance Targets

- **WASM Mode**: <200ms latency, 10-15 FPS, <50% CPU
- **Server Mode**: <100ms latency, 20-30 FPS, <70% CPU


Hereâ€™s a **complete cheat sheet** of all the different ways you can run your WebRTC detection project, covering **local Node.js**, **Docker**, **mode switching**, and **ngrok support**:

---

## **1ï¸âƒ£ Local Node.js (without Docker)**

| Purpose                           | Command                          | Notes                                             |
| --------------------------------- | -------------------------------- | ------------------------------------------------- |
| WASM mode (default, low-resource) | `./start.sh`                     | Runs inference in the browser using WASM.         |
| WASM mode explicitly              | `MODE=wasm ./start.sh`           | Forces WASM mode even if default changes.         |
| Server mode (Node.js inference)   | `MODE=server ./start.sh`         | Uses server-side ONNX inference.                  |
| WASM mode + ngrok                 | `./start.sh --ngrok`             | Exposes localhost to phone for remote connection. |
| Server mode + ngrok               | `MODE=server ./start.sh --ngrok` | Server-side inference with remote access.         |

---

## **2ï¸âƒ£ Docker / Docker Compose**

Your `docker-compose.yml` already sets:

```yaml
environment:
  - MODE=wasm
```

* âœ… Default: **WASM mode**.

| Purpose                    | Command                                                                          | Notes                               |
| -------------------------- | -------------------------------------------------------------------------------- | ----------------------------------- |
| Default WASM mode          | `docker-compose up --build`                                                      | Uses `MODE=wasm` from compose file. |
| Server mode                | `docker-compose run -e MODE=server webrtc-detection`                             | Overrides default mode.             |
| Server mode with rebuild   | `docker-compose run -e MODE=server --build webrtc-detection`                     | Ensures latest code & dependencies. |
| Expose ngrok inside Docker | Modify `start.sh` inside container with `--ngrok` OR use local Node.js for ngrok | Easier to run ngrok outside Docker. |

---

## **3ï¸âƒ£ Bench / Metrics collection**

| Purpose            | Command                                            | Notes                          |
| ------------------ | -------------------------------------------------- | ------------------------------ |
| WASM bench (30s)   | `./bench/run_bench.sh --duration 30 --mode wasm`   | Outputs `metrics.json`.        |
| Server bench (30s) | `./bench/run_bench.sh --duration 30 --mode server` | Server-side inference metrics. |

---

### **Summary Table**

| Run Environment | Mode   | ngrok | Command                                              |
| --------------- | ------ | ----- | ---------------------------------------------------- |
| Local Node      | WASM   | No    | `./start.sh`                                         |
| Local Node      | WASM   | Yes   | `./start.sh --ngrok`                                 |
| Local Node      | Server | No    | `MODE=server ./start.sh`                             |
| Local Node      | Server | Yes   | `MODE=server ./start.sh --ngrok`                     |
| Docker          | WASM   | No    | `docker-compose up --build`                          |
| Docker          | Server | No    | `docker-compose run -e MODE=server webrtc-detection` |

---

âœ… **Key notes**

* `MODE` is the main switch between **low-resource WASM** and **server inference**.
* ngrok is optional for remote phone access.
* Docker defaults to WASM unless overridden.
* Bench scripts are independent of Docker; they read the `--mode` argument.


ðŸ“± Phone Browser          ðŸ’» Desktop Browser         ðŸ–¥ï¸ Server
     â”‚                         â”‚                      â”‚
     â”‚ â”€â”€â”€â”€ WebRTC Video â”€â”€â”€â”€â–º â”‚                      â”‚
     â”‚                         â”‚                      â”‚
     â”‚                         â”‚ â”€â”€ Socket.IO â”€â”€â”€â”€â”€â”€â–º â”‚
     â”‚                         â”‚   (frame-data)       â”‚
     â”‚                         â”‚                      â”‚
     â”‚                         â”‚                      â–¼
     â”‚                         â”‚              server/inference.js
     â”‚                         â”‚                   (YOLO)
     â”‚                         â”‚                      â”‚
     â”‚                         â”‚ â—„â”€â”€ Socket.IO â”€â”€â”€â”€â”€â”€ â”‚
     â”‚                         â”‚   (detections)       â”‚
     â”‚                         â”‚                      â”‚
     â”‚                         â–¼                      â”‚
     â”‚                  client/app.js                 â”‚
     â”‚                 (overlay drawing)              â”‚



Complete Data Flow Sequence
WASM Mode Data Flow:
1. ðŸ“± Phone Browser:
   â””â”€â”€ navigator.mediaDevices.getUserMedia()
       â””â”€â”€ WebRTC stream to desktop

2. ðŸ’» Desktop Browser (client/app.js):
   â”œâ”€â”€ captureAndSendFrame()
   â”‚   â”œâ”€â”€ canvas.drawImage(video)
   â”‚   â””â”€â”€ canvas.toDataURL() â†’ base64 image
   â”‚
   â””â”€â”€ this.wasmInference.detect(imageData)
       â”‚
       â–¼
3. ðŸ§  WASM Inference (client/wasm-inference.js):
   â”œâ”€â”€ preprocessImage() â†’ 320x320 tensor
   â”œâ”€â”€ session.run() â†’ ONNX inference
   â”œâ”€â”€ postprocessResults() â†’ NMS + format
   â””â”€â”€ return detections[]
       â”‚
       â–¼
4. ðŸ“Š Overlay (client/app.js):
   â””â”€â”€ drawDetections() â†’ canvas overlay



Server Mode Data Flow:
1. ðŸ“± Phone Browser:
   â””â”€â”€ navigator.mediaDevices.getUserMedia()
       â””â”€â”€ WebRTC stream to desktop

2. ðŸ’» Desktop Browser (client/app.js):
   â”œâ”€â”€ captureAndSendFrame()
   â”‚   â”œâ”€â”€ canvas.drawImage(video)
   â”‚   â””â”€â”€ canvas.toDataURL() â†’ base64 image
   â”‚
   â””â”€â”€ socket.emit('frame-data', frameData)
       â”‚
       â–¼
3. ðŸ–¥ï¸ Server (server/index.js):
   â”œâ”€â”€ socket.on('frame-data')
   â””â”€â”€ yoloInference.detect(data)
       â”‚
       â–¼
4. ðŸ§  Server Inference (server/inference.js):
   â”œâ”€â”€ preprocessImage() â†’ 640x640 tensor
   â”œâ”€â”€ session.run() â†’ ONNX inference
   â”œâ”€â”€ postprocessResults() â†’ NMS + format
   â””â”€â”€ return detections[]
       â”‚
       â–¼
5. ðŸ–¥ï¸ Server (server/index.js):
   â””â”€â”€ socket.emit('detections', result)
       â”‚
       â–¼
6. ðŸ’» Desktop Browser (client/app.js):
   â”œâ”€â”€ socket.on('detections')
   â””â”€â”€ drawDetections() â†’ canvas overlay



ðŸ“‚ File Dependencies Map
start.sh
    â”‚
    â””â”€â”€ npm start
        â”‚
        â””â”€â”€ server/index.js (MAIN ENTRY)
            â”œâ”€â”€ imports: server/inference.js
            â”œâ”€â”€ serves: client/index.html
            â”œâ”€â”€ serves: client/app.js
            â”œâ”€â”€ serves: client/wasm-inference.js
            â””â”€â”€ serves: public/models/yolov5n.onnx
                â”‚
                â”œâ”€â”€ client/app.js
                â”‚   â”œâ”€â”€ uses: Socket.IO connection
                â”‚   â”œâ”€â”€ imports: client/wasm-inference.js
                â”‚   â””â”€â”€ renders: HTML canvas overlay
                â”‚
                â”œâ”€â”€ client/wasm-inference.js
                â”‚   â”œâ”€â”€ loads: public/models/yolov5n.onnx
                â”‚   â””â”€â”€ uses: ONNX Runtime Web (CDN)
                â”‚
                â””â”€â”€ server/inference.js
                    â”œâ”€â”€ loads: server/models/yolov5n.onnx
                    â””â”€â”€ uses: onnxruntime-node


                    
ðŸŽ¯ Key Data Structures
Frame Data (Browser â†’ Server):
{
    frame_id: 1692000000123,
    capture_ts: 1692000000000,
    image_data: "data:image/jpeg;base64,/9j/4AAQ..." // Base64
}
Detection Result (Server â†’ Browser):
{
    frame_id: 1692000000123,
    capture_ts: 1692000000000,
    recv_ts: 1692000000100,
    inference_ts: 1692000000120,
    detections: [
        {
            label: "person",
            score: 0.93,
            xmin: 0.12, ymin: 0.08,
            xmax: 0.34, ymax: 0.67
        }
    ]
}